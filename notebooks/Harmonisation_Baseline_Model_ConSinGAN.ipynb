{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X4-kKK_1yPx"
      },
      "source": [
        "## Clone Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1cDHVEb1uOh",
        "outputId": "eaf6185e-a93a-4930-bd23-c8e114cf147d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Adversarial-Representation-Learning-for-Medical-Imaging'...\n",
            "remote: Enumerating objects: 503, done.\u001b[K\n",
            "remote: Counting objects: 100% (503/503), done.\u001b[K\n",
            "remote: Compressing objects: 100% (354/354), done.\u001b[K\n",
            "remote: Total 503 (delta 289), reused 347 (delta 137), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (503/503), 49.63 MiB | 10.83 MiB/s, done.\n",
            "Resolving deltas: 100% (289/289), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repo\n",
        "!git clone https://<DRIVE>@github.com/java-master007/Adversarial-Representation-Learning-for-Medical-Imaging.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UgmJZbs11J7",
        "outputId": "f7634ed2-8d60-4183-bddc-ef91dbfd24fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Adversarial-Representation-Learning-for-Medical-Imaging\n"
          ]
        }
      ],
      "source": [
        "# Change to the correct directory\n",
        "%cd Adversarial-Representation-Learning-for-Medical-Imaging/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vab1bbrK13JC"
      },
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "# It will need restart on colab\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-EZvBH-LlrR"
      },
      "source": [
        "## Image Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGNiTcN3w6pq"
      },
      "source": [
        "Requires that:\n",
        "- malign.png be 3-channel\n",
        "- normal.png be 3-channel\n",
        "- malign_mask.png be one-channel (BINARY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KFcbV3kZcJih"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import cv2\n",
        "import os\n",
        "from skimage import io as img\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.image as mpimg\n",
        "import torch\n",
        "import torchvision as tv\n",
        "from PIL import Image, ImageDraw, ImageFilter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_laterality(image):\n",
        "    left_edge = np.sum(image[:, 0])  \n",
        "    right_edge = np.sum(image[:, -1])\n",
        "    return (True, False) if left_edge < right_edge else (False, True)"
      ],
      "metadata": {
        "id": "CXoEsj5DDRTx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_measures(image):\n",
        "    positions = np.nonzero(image)\n",
        "    top = positions[0].min()\n",
        "    bottom = positions[0].max()\n",
        "    left = positions[1].min()\n",
        "    right = positions[1].max()\n",
        "    return top, right, bottom, left"
      ],
      "metadata": {
        "id": "PdQW6pLMDSnn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_start_coordinate(image):\n",
        "    positions = np.nonzero(image)\n",
        "    bottom = positions[0].max()\n",
        "    x_bottom = int(np.mean(np.nonzero(image[bottom])))\n",
        "    return x_bottom, bottom"
      ],
      "metadata": {
        "id": "_W5Yy675DVJ2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correct_value(number):\n",
        "    if number == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "mXNqcyeaDWew"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_binary(image, pth):\n",
        "    b_image = []\n",
        "    for arr in image:\n",
        "        curr = [get_correct_value(elem) for elem in arr]\n",
        "        b_image.append(curr)\n",
        "    b_image = np.array(b_image, dtype=np.uint8)\n",
        "\n",
        "    plt.imsave(pth, np.array(b_image), cmap=cm.gray)\n",
        "    return b_image"
      ],
      "metadata": {
        "id": "kuQQcIOdDXc_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def does_collage_mask(width, height, malign, normal):\n",
        "    \n",
        "    # Crop both the mass, and the normal\n",
        "    crop_segmentation(malign, 'malign_aux.png')\n",
        "\n",
        "    normal_image = Image.open(normal)\n",
        "    mass_to_paste = Image.open('malign_aux.png')\n",
        "\n",
        "    # Creates collage and save\n",
        "    back_im = normal_image.copy()\n",
        "    back_im.paste(mass_to_paste, (width,height), mass_to_paste)\n",
        "    \n",
        "    return list(back_im.getdata()) == list(normal_image.getdata())"
      ],
      "metadata": {
        "id": "RKbP8PApDcPh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_collage_possible(malign_mask_pth, normal_breast_pth):\n",
        "\n",
        "  # Operations Threshold\n",
        "  threshold = 50\n",
        "\n",
        "  # Read the images\n",
        "  malign_mask = cv2.imread(malign_mask_pth, cv2.IMREAD_GRAYSCALE)\n",
        "  normal_breast = cv2.imread(normal_breast_pth, cv2.IMREAD_GRAYSCALE)\n",
        "  _, normal_x = normal_breast.shape\n",
        "  normal_breast = image_to_binary(normal_breast, '/content/normal_aux.png')\n",
        "\n",
        "  # Get images laterality\n",
        "  R, _ = get_image_laterality(normal_breast)\n",
        "\n",
        "  # Get images measures\n",
        "  # Calculate malign mass measures\n",
        "  m_top, m_right, m_bottom, m_left = get_measures(malign_mask)\n",
        "\n",
        "  # Calculate normal breast measures\n",
        "  n_top, n_right, n_bottom, n_left = get_measures(normal_breast)\n",
        "\n",
        "  # Calculate widths and heights\n",
        "  malign_mass_width = abs(m_right-m_left)\n",
        "  malign_mass_height = abs(m_bottom-m_top)\n",
        "  normal_breast_width = abs(n_right-n_left)\n",
        "  normal_breast_height = abs(n_bottom-n_top)\n",
        "\n",
        "  # Check if its worth the try\n",
        "  if malign_mass_width > normal_breast_width or malign_mass_height > normal_breast_height:\n",
        "    return -1, -1\n",
        "\n",
        "  # Get bottom base coordinate\n",
        "  bottom_coordinate = get_start_coordinate(normal_breast)\n",
        "\n",
        "  # Coordinate collage starts bottom\n",
        "  c, d = bottom_coordinate\n",
        "\n",
        "  if R:\n",
        "\n",
        "    # Check if mass is all inside image. If not, then go left + threshold\n",
        "    if normal_x - c < malign_mass_width:\n",
        "      c, d = c-(malign_mass_width-(normal_x - c)+threshold), d\n",
        "\n",
        "    # Go up the height plus the threshold\n",
        "    c, d = c, d-(malign_mass_height+threshold)\n",
        "\n",
        "    # Go up until the masks match. If never match then skip them\n",
        "    while d > threshold:\n",
        "      if does_collage_mask(c, d, malign_mask_pth, '/content/normal_aux.png'):\n",
        "        return c, d\n",
        "\n",
        "      c, d = c, d-threshold\n",
        "\n",
        "    return -1, -1\n",
        "  else:\n",
        "    \n",
        "    # Check if mass is all inside image. If not, then go right + threshold\n",
        "    if c < malign_mass_width:\n",
        "      c, d = c+(malign_mass_width-c+threshold), d\n",
        "\n",
        "    # Go up the height plus the threshold\n",
        "    c, d = c, d-(malign_mass_height+threshold)\n",
        "\n",
        "    # Go up until the masks match. If never match then skip them\n",
        "    while d > threshold:\n",
        "      if does_collage_mask(c, d, malign_mask_pth, '/content/normal_aux.png'):\n",
        "        return c, d\n",
        "\n",
        "      c, d = c, d-threshold\n",
        "\n",
        "    return -1, -1"
      ],
      "metadata": {
        "id": "GtPk4p5gDiBj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "7EvfVGz8cK37"
      },
      "outputs": [],
      "source": [
        "# Remove the 4 channel to collage image\n",
        "def remove_4_channel(im_path, output_path):\n",
        "\n",
        "    img = cv2.imread(im_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    # Transpose naive image to properly see it\n",
        "    tranposed = img.transpose(2,0,1)\n",
        "\n",
        "    # Transpose image again with only the 3 rgb channels to save\n",
        "    output = tranposed[0:3].transpose(1,2,0)\n",
        "\n",
        "    # Save new naive image (3-channels)\n",
        "    cv2.imwrite(output_path, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PGT3kdfwcNzb"
      },
      "outputs": [],
      "source": [
        "# Resize image for hamronisation\n",
        "def resize_image(im_path, percent_original, output_path):\n",
        "    img = cv2.imread(im_path, cv2.IMREAD_UNCHANGED)\n",
        "    \n",
        "    print('Original Dimensions : ',img.shape)\n",
        "    \n",
        "    scale_percent = percent_original # percent of original size\n",
        "    width = int(img.shape[1] * scale_percent / 100)\n",
        "    height = int(img.shape[0] * scale_percent / 100)\n",
        "    dim = (width, height)\n",
        "    \n",
        "    # resize image\n",
        "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "    \n",
        "    print('Resized Dimensions : ',resized.shape)\n",
        "    cv2.imwrite(output_path, resized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2TkjI3EXcMVq"
      },
      "outputs": [],
      "source": [
        "# Make mask have 3 channels\n",
        "def make_3_channels_mask(im_path, out_path):\n",
        "  i = img.imread(im_path)\n",
        "  new_i = []\n",
        "  new_i.append(i)\n",
        "  new_i.append(i)\n",
        "  new_i.append(i)\n",
        "  new_i = torch.tensor(np.array(new_i))\n",
        "  tv.io.write_png(new_i, out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bZX9tjIyXxmQ"
      },
      "outputs": [],
      "source": [
        "# Crops the segmentation by its limits\n",
        "def crop_segmentation(fp, outp):\n",
        "  imag = cv2.imread(fp, cv2.IMREAD_UNCHANGED)\n",
        "  imageObject = Image.open(fp)\n",
        "  positions = np.nonzero(imag)\n",
        "\n",
        "  top = positions[0].min()\n",
        "  bottom = positions[0].max()\n",
        "  left = positions[1].min()\n",
        "  right = positions[1].max()\n",
        "\n",
        "  cropped = imageObject.crop((left,top,right,bottom))\n",
        "  cropped.save(outp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "pk4uvtD9T-Kz"
      },
      "outputs": [],
      "source": [
        "# Makes a collage given the malign image, the malign mask, and the normal image\n",
        "def make_collage(malign_pth, malign_mask_pth, normal_pth, width, height):\n",
        "\n",
        "  # Reads malign base image\n",
        "  malign = cv2.imread(malign_pth, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "  # Convert mask to 3 channels\n",
        "  make_3_channels_mask(malign_mask_pth, '/content/malign_mask3.png')\n",
        "  malign_mask = cv2.imread('/content/malign_mask3.png', cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "  # Grab the image mask from the mass image\n",
        "  masked = malign.copy()\n",
        "  masked[malign_mask == 0] = 0\n",
        "  cv2.imwrite('/content/segmented_mass.png', masked)\n",
        "\n",
        "  # Crop both the mask, and the masked mass\n",
        "  crop_segmentation('/content/segmented_mass.png', '/content/cropped_mass.png')\n",
        "  crop_segmentation(malign_mask_pth, '/content/malign_mask_cropped.png')\n",
        "\n",
        "  normal_image = Image.open(normal_pth)\n",
        "  mass_to_paste = Image.open('/content/cropped_mass.png')\n",
        "  mass_mask = Image.open('/content/malign_mask_cropped.png')\n",
        "\n",
        "  # Creates collage and save\n",
        "  back_im = normal_image.copy()\n",
        "  #TODO: Calculate how to paste\n",
        "  back_im.paste(mass_to_paste, (width,height), mass_mask)\n",
        "  back_im.save('/content/collage.png', quality=95)\n",
        "\n",
        "  # Creates collage mask\n",
        "  collage_mask = Image.new(\"L\", back_im.size, 0)\n",
        "  collage_mask.paste(mass_mask, (width,height))\n",
        "  collage_mask.save('/content/collage_mask.png', quality=95)\n",
        "\n",
        "  # Deletes unecessary images\n",
        "  try:\n",
        "    os.remove('/content/malign_mask3.png')\n",
        "    os.remove('/content/segmented_mass.png')\n",
        "    os.remove('/content/cropped_mass.png')\n",
        "    os.remove('/content/malign_mask_cropped.png')\n",
        "  except OSError as e:\n",
        "    print(f\"FAILED\\nFile: {e.filename}\\nError: {e.strerror}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w, h = is_collage_possible(malign_mask_pth='malign_mask.png', normal_breast_pth='normal.png')\n",
        "w, h"
      ],
      "metadata": {
        "id": "TBlMM1JrWLb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4edcb73-a057-4dc0-8562-0e2e50a6e3c0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1476, 1060)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "RKsyHncdbqI9"
      },
      "outputs": [],
      "source": [
        "make_collage(malign_pth='/content/malign.png', malign_mask_pth='/content/malign_mask.png', normal_pth='/content/normal.png', width=w, height=h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHsKlSIk1_-6"
      },
      "source": [
        "## Harmonizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcUgsHrZ1CMK"
      },
      "source": [
        "### Harmonzer Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R5F5Tzc2qdB",
        "outputId": "57629445-fb66-4088-a064-80eb8bcff020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN\n"
          ]
        }
      ],
      "source": [
        "# Change to correct directory\n",
        "%cd MedSinGAN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fgEKbQamS5c"
      },
      "outputs": [],
      "source": [
        "# Make the collage mask 3-channel\n",
        "make_3_channels_mask('/content/collage_mask.png', '/content/collage_mask3.png')\n",
        "os.remove('/content/collage_mask.png')\n",
        "os.rename('/content/collage_mask3.png', '/content/collage_mask.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGuzzV6l2BIZ",
        "outputId": "64057370-b3b3-4199-bf76-edada6c80982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model (TrainedModels/normal/2022_01_21_15_01_07_harmonization_niter_3000_lr_scale_0.1_nstages_2_BN_act_lrelu_0.3)\n",
            "Training model with the following parameters:\n",
            "\t number of stages: 2\n",
            "\t number of concurrently trained stages: 3\n",
            "\t learning rate scaling: 0.1\n",
            "\t non-linearity: lrelu\n",
            "Training on image pyramid: [torch.Size([1, 3, 148, 120]), torch.Size([1, 3, 250, 204])]\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
            "  FutureWarning,\n",
            "stage [0/1]:: 100% 3000/3000 [34:25<00:00,  1.45it/s]\n",
            "stage [1/1]:: 100% 3000/3000 [52:26<00:00,  1.05s/it]\n",
            "Time for training: 5221.880122661591 seconds\n"
          ]
        }
      ],
      "source": [
        "# Normal breast collage Harmonizer creation\n",
        "!python main_train.py --train_mode harmonization --gpu 0 --train_stages 2 --im_min_size 120 --lrelu_alpha 0.3 --niter 3000 --batch_norm --input_name /content/normal.png --naive_img /content/collage.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMNlBMrG1Hmg"
      },
      "source": [
        "### Fine-Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jx-vpMU5osX"
      },
      "outputs": [],
      "source": [
        "# Get the latest model\n",
        "def get_latest_model():\n",
        "  base_path = \"TrainedModels/normal/\"\n",
        "  models = os.listdir(base_path)\n",
        "\n",
        "  latest = 0 # Values will always be bigger than 0\n",
        "  desired = models[0]\n",
        "\n",
        "  for id, model in enumerate(models):\n",
        "    splitted = model.split(\"_\")\n",
        "    code = splitted[:6]\n",
        "    code = int(''.join(code))\n",
        "    if code > latest:\n",
        "      latest = code\n",
        "      desired = model\n",
        "\n",
        "  return os.path.join(base_path, desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AacGNG07vBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ae28c8-724e-4546-aecf-b07145135a04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# FINE TUNE\n",
        "m = get_latest_model()\n",
        "fine_tune_cmd = \"python main_train.py --gpu 0 --train_mode harmonization --input_name /content/normal.png --naive_img /content/collage.png --fine_tune --model_dir \" + str(m)\n",
        "os.system(fine_tune_cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBaDW1M0gG7u"
      },
      "outputs": [],
      "source": [
        "# FINE TUNE\n",
        "# !python main_train.py --gpu 0 --train_mode harmonization --input_name /content/normal.png --naive_img /content/collage.png --fine_tune --model_dir TrainedModels/normal/2022_01_17_19_51_18_harmonization_niter_1000_lr_scale_0.1_nstages_8_BN_act_lrelu_0.3 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjOZzo8S18Z7"
      },
      "source": [
        "### Harmonise The Naive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WbYrgQoBdzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438c3c37-c982-4734-c9c6-2f57c35859e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Normal breast collage harmonisation\n",
        "m = get_latest_model()\n",
        "harmonise_cmd = \"python evaluate_model.py --gpu 0 --model_dir \" + str(m) + \" --naive_img /content/collage.png\"\n",
        "os.system(harmonise_cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR3plhis-83m",
        "outputId": "708529ac-8e7c-4549-b788-ac8ae0906c59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Normal breast collage harmonisation\n",
        "#!python evaluate_model.py --gpu 0 --model_dir TrainedModels/normal/2022_01_17_21_07_38_harmonization_fine-tune_niter_2000_lr_scale_0.1_nstages_8_BN_act_lrelu_0.3 --naive_img /content/collage.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBSXbWo5A-Ck"
      },
      "source": [
        "### Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNBvrMZ8DYaG"
      },
      "outputs": [],
      "source": [
        "# Resizes an image to a specific dimension\n",
        "def resize_to_dim(img_pth, width, height, out_pth):\n",
        "  base = cv2.imread(img_pth, cv2.IMREAD_UNCHANGED)\n",
        "  dim = (width, height)\n",
        "  resized = cv2.resize(base, dim)\n",
        "  cv2.imwrite(out_pth, resized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6zP9J5ZD8sP"
      },
      "outputs": [],
      "source": [
        "resize_to_dim('/content/normal.png', 204, 250, '/content/normal_resized.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "38ae8b25e4ca4d918c4917b2b1de8698",
            "7b2602b63edc4b7da86c3507f6a811f7",
            "e967d143967a4c8d8a194042d6bc3cc2",
            "7b39b3053d2a47d1bb579bae64267b49",
            "abafeadc04304c5eb7acdb89851259b1",
            "b0e8f17ddf6e49be973f402b8c79a893",
            "f081e02284684bb6ba2d18e77dc5b633",
            "bc93d93e7c284173acffb53b7589741f",
            "d07251133fcc42ef9e6aec9235daa070",
            "21e22ce7aa424124990d16a8ca7ce332",
            "8b7a8938c2314063b61f84fd89619490",
            "3eff97f5128c4720ae7871f92dbb3fc0",
            "770b05100c0d4a7eafe150e627c81af3",
            "16e5c4d4fa1243ac84379a0ece6bf482",
            "a6163bd2d7c7416baa636f4d63978b3e",
            "e67679cbd82d497da445f424d6025a6b",
            "61cd02c543e64c3bb7574798e6a4f38b",
            "c32464917d674cfcb3f400d9efd1e3e1",
            "499b456475a64a7ca06c917fdbd3c317",
            "fbaeb21ced42486ea02c5203ca67d13f",
            "2821f56ec59245a0ae3db144c524b39b",
            "625604035509420c8f4c89d414a69beb"
          ]
        },
        "id": "5YlWVtyXBAU8",
        "outputId": "1d2c7b14-1545-4cd2-b306-3a1d77a83d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38ae8b25e4ca4d918c4917b2b1de8698",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/richzhang/PerceptualSimilarity/raw/master/lpips/weights/v0.1/alex.pth\" to /root/.cache/torch/hub/checkpoints/alex.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eff97f5128c4720ae7871f92dbb3fc0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/5.87k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LPIPS: 0.10238905996084213\n",
            "SSIM: 0.8805417418479919\n",
            "MS-SSIM: 0.9281067252159119\n"
          ]
        }
      ],
      "source": [
        "import evaluate_generation\n",
        "\n",
        "base_img = '/content/normal_resized.png'\n",
        "eval_folder = os.path.join(get_latest_model(),'Evaluation_/content/collage.png')\n",
        "\n",
        "evaluator = evaluate_generation.GenerationEvaluator(base_img, eval_folder)\n",
        "\n",
        "lpips = evaluator.run_lpips()\n",
        "ssim, ms_ssim = evaluator.run_mssim()\n",
        "print(f\"LPIPS: {lpips}\\nSSIM: {ssim}\\nMS-SSIM: {ms_ssim}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ds0KPgz6uI-"
      },
      "source": [
        "## Save Harmonizer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZS5yU5U6vkU"
      },
      "outputs": [],
      "source": [
        "# Import files to download zips\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTxipCzy60ge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "696be1be-af1c-4528-873f-52b3306294e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/.trash/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/tags/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/tags/mlflow.user (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/tags/mlflow.source.name (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/tags/mlflow.source.git.commit (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/tags/mlflow.runName (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/params/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/params/N Training Stages (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/params/Activation Function (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/params/Train Depth (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/params/N Iterations (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/params/Learning Scale Rate (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/metrics/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/metrics/Generator Train Loss (deflated 56%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/metrics/Discriminator Train Loss Real (deflated 56%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/metrics/Discriminator Train Loss Gradient Penalty (deflated 57%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/metrics/Generator Train Loss Reconstruction (deflated 57%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/metrics/Discriminator Train Loss Fake (deflated 55%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/meta.yaml (deflated 41%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/2905951eb95e4cfcb0a1e4a69118f940/artifacts/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/meta.yaml (deflated 17%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/tags/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/tags/mlflow.user (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/tags/mlflow.source.name (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/tags/mlflow.source.git.commit (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/tags/mlflow.runName (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/params/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/params/N Training Stages (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/params/Activation Function (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/params/Train Depth (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/params/N Iterations (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/params/Learning Scale Rate (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/metrics/ (stored 0%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/metrics/Generator Train Loss (deflated 51%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/metrics/Discriminator Train Loss Real (deflated 51%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/metrics/Discriminator Train Loss Gradient Penalty (deflated 53%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/metrics/Generator Train Loss Reconstruction (deflated 54%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/metrics/Discriminator Train Loss Fake (deflated 51%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/meta.yaml (deflated 41%)\n",
            "  adding: content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns/0/1a47623adbd3407493c7b965a57ef67f/artifacts/ (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b4b7b78a-308a-4987-956f-43f152527f6f\", \"mlrun.zip\", 22029)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Zip the mlruns metrics to analyse\n",
        "!zip -r /content/mlrun.zip /content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns\n",
        "files.download(\"/content/mlrun.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "o-IZgTDG610d",
        "outputId": "98b7c3a5-7aee-4223-a0e4-bdd3a4631d14"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3acb960a-82f3-4a53-b43f-191b0d983a03\", \"best_harmonisation_model.zip\", 28370721)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Zip the best model analysed based on the mlruns\n",
        "m = get_latest_model()\n",
        "zip_cmd = \"zip -r /content/best_harmonisation_model.zip \" + str(m)\n",
        "os.system(zip_cmd)\n",
        "files.download(\"/content/best_harmonisation_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fEUmlAKc7zn"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tip34yzOc8Yo"
      },
      "outputs": [],
      "source": [
        "# Remove the MLFlow runs\n",
        "! rm -r /content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/mlruns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RydIJkA9dDav"
      },
      "outputs": [],
      "source": [
        "# Remove all trained models\n",
        "! rm -r /content/Adversarial-Representation-Learning-for-Medical-Imaging/MedSinGAN/TrainedModels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-YrF4bB_dFAz"
      },
      "outputs": [],
      "source": [
        "# Remove all images\n",
        "! find . -name \"*.png\" -type f -delete"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TcjWDV8lbgXR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9X4-kKK_1yPx",
        "dHsKlSIk1_-6",
        "6Ds0KPgz6uI-",
        "0fEUmlAKc7zn"
      ],
      "name": "Harmonisation Baseline Model - ConSinGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38ae8b25e4ca4d918c4917b2b1de8698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7b2602b63edc4b7da86c3507f6a811f7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e967d143967a4c8d8a194042d6bc3cc2",
              "IPY_MODEL_7b39b3053d2a47d1bb579bae64267b49",
              "IPY_MODEL_abafeadc04304c5eb7acdb89851259b1"
            ]
          }
        },
        "7b2602b63edc4b7da86c3507f6a811f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e967d143967a4c8d8a194042d6bc3cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0e8f17ddf6e49be973f402b8c79a893",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f081e02284684bb6ba2d18e77dc5b633"
          }
        },
        "7b39b3053d2a47d1bb579bae64267b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc93d93e7c284173acffb53b7589741f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244408911,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244408911,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d07251133fcc42ef9e6aec9235daa070"
          }
        },
        "abafeadc04304c5eb7acdb89851259b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_21e22ce7aa424124990d16a8ca7ce332",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [00:01&lt;00:00, 136MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b7a8938c2314063b61f84fd89619490"
          }
        },
        "b0e8f17ddf6e49be973f402b8c79a893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f081e02284684bb6ba2d18e77dc5b633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc93d93e7c284173acffb53b7589741f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d07251133fcc42ef9e6aec9235daa070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21e22ce7aa424124990d16a8ca7ce332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b7a8938c2314063b61f84fd89619490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3eff97f5128c4720ae7871f92dbb3fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_770b05100c0d4a7eafe150e627c81af3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_16e5c4d4fa1243ac84379a0ece6bf482",
              "IPY_MODEL_a6163bd2d7c7416baa636f4d63978b3e",
              "IPY_MODEL_e67679cbd82d497da445f424d6025a6b"
            ]
          }
        },
        "770b05100c0d4a7eafe150e627c81af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16e5c4d4fa1243ac84379a0ece6bf482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_61cd02c543e64c3bb7574798e6a4f38b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c32464917d674cfcb3f400d9efd1e3e1"
          }
        },
        "a6163bd2d7c7416baa636f4d63978b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_499b456475a64a7ca06c917fdbd3c317",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 6009,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 6009,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbaeb21ced42486ea02c5203ca67d13f"
          }
        },
        "e67679cbd82d497da445f424d6025a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2821f56ec59245a0ae3db144c524b39b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.87k/5.87k [00:00&lt;00:00, 157kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_625604035509420c8f4c89d414a69beb"
          }
        },
        "61cd02c543e64c3bb7574798e6a4f38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c32464917d674cfcb3f400d9efd1e3e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "499b456475a64a7ca06c917fdbd3c317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbaeb21ced42486ea02c5203ca67d13f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2821f56ec59245a0ae3db144c524b39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "625604035509420c8f4c89d414a69beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}